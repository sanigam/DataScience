---
title: "Learning from Imbalanced Data"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    theme: spacelab
---

# Install and Load Relevant Packages
Set up your working directory to the current Rmd source file location. Also, make sure that the data `ticdata.rda` is located under `../data` subdirectory relative to the location of this Rmd file.
```{r}
#setwd("directory_path_to_Rmd_source_file")
setwd("~/Desktop/DS/AMLHW")
#setwd("F:/Samatova_EXT/0-LECTURES/0-NCSU-CISCO-DS/Fall-2016/AdvancedML/Learning.ImbalancedData/codes")
```

Here are some packages you will probably want to load as well as some helper functions. You may have to install these packages if you haven't before. 
```{r message=FALSE}
source("./libraries.R")
source("./functions.R")
```

# Case Study: Predicting Insurance Policy Ownership
A data set is generated by the computational inteligence and learning (CoIL) research network to illlustrate methods for combating  class imbalance as part of the 2000 CoIL Challenge.

The outcome is whether the consumer purchased the insurance. It is a highly unbalanced data with only 6% of customers having purchased policies. There are 85 predictors that consist of:

* Customer subtype designation, such as "Traditional Families" or "Affluent Young Families"
* Demongraphic factors: religion, education level, social class, income. Customers residing in the same zip code are assumed to have the same values for the attributes
* Product owndership information such as the contribution to other policies

```{r}
# Assume that your working directory is set to where the Rmd file is
# and the dataset is in the directory data above this directory
load(file="../data/ticdata.rda")
# View(ticdata)
# names(ticdata)
```

# Preprocess the Data

Identify the columns that are regular factors and those that are ordered factors.

```{r}
isOrdered <- unlist(lapply(ticdata, is.ordered))
isFactor <- unlist(lapply(ticdata, is.factor))
convertCols <- names(isOrdered)[isOrdered | isFactor]
convertCols <- convertCols[convertCols != "CARAVAN"]
```

Many of the factor levels have non-standard characters, such as "%", commas, or other values. When they are converted to dummy variable columns, the values violate the rules of naming the variables. To overcome this issue, such names should be re-encoded to make them more simplistic. This is achieved using the helper function, `recodeLevels`. 
```{r}
for(i in convertCols) {
  ticdata[,i] <- recodeLevels(ticdata[,i])
}
##ticdata$CARAVAN <- factor(as.character(ticdata$CARAVAN),
##                    levels = rev(levels(ticdata$CARAVAN)))
```

# Create Training, Evaluation, and Testing Data Sets

Create training, evaluation and testing data sets using stratified random sampling:
* *training* (*n*=6877): The data to be used for building the model, estimating model parameters, tuning models, etc.
* *evaluation* (*n*=983): The set to be used for post-processing steps such as alternative probability cut-offs
* *testing* (*n*=1962): The set to be used solely for final evaluations of the models

```{r}
set.seed(156)
split1 <- createDataPartition(ticdata$CARAVAN, p = .7)[[1]]
other <- ticdata[-split1,]
training <- ticdata[ split1,]
set.seed(934)
split2 <- createDataPartition(other$CARAVAN, p = 1/3)[[1]]
evaluation <- other[ split2,]
testing <- other[-split2,]
```

Make sure that the class imbalance is consistent in all the sets, namely they have roughly the same rate of customers with the insurance (about 6%) .
```{r}
options(digits=3)
(summary(training$CARAVAN) / length(training$CARAVAN))*100
(summary(evaluation$CARAVAN) / length(evaluation$CARAVAN))*100
(summary(testing$CARAVAN) / length(testing$CARAVAN))*100
```

# Create Dummy Variables
Dummy variables are useful for several models being fit.
The randomForest function has a limitation that all factor predictors must
not have more than 32 levels. The customer type predictor has 39 levels, so
a predictor set of dummy variables is created for this using
the `model.matrix()` function:

```{r}
## The first column is the intercept, which is eliminated:
trainingInd <- data.frame(model.matrix(CARAVAN ~ .,
                    data = training))[,-1]
evaluationInd <- data.frame(model.matrix(CARAVAN ~ .,
                    data = evaluation))[,-1]
testingInd <- data.frame(model.matrix(CARAVAN ~ .,
                    data = testing))[,-1]
```

Add the outcome back into the data set:
```{r}
trainingInd$CARAVAN <- training$CARAVAN
evaluationInd$CARAVAN <- evaluation$CARAVAN
testingInd$CARAVAN <- testing$CARAVAN
dim(trainingInd)
dim(evaluationInd)
dim(testingInd)
```

Determine a predictor set without highly sparse and unbalanced distributions. The predictors that are too sparse will be excluded during model building.
```{r}
isNZV <- nearZeroVar(trainingInd)
noNZVSet <- names(trainingInd)[-isNZV]
```

# Set up Control Functions 
Create control functions for situations when class probabilities
can be created and when they cannot:

```{r}
final.n.folds <- 10
#final.n.folds <- 3
ctrl <- trainControl(method = "cv", 
          number = final.n.folds,
          classProbs = TRUE,
          summaryFunction = fiveStats,
          verboseIter = TRUE)
ctrlNoProb <- ctrl
ctrlNoProb$summaryFunction <- fourStats
ctrlNoProb$classProbs <- FALSE
```

# Build Logistic Regression and Random Forest Models

The final *random forest model* uses 1500 trees in the forest and tuned over 5 values of the *mtry* parameter (with an optimal value of *mtry*=126). The final *logistic regression model* utilizes a simple additive model, i.e., no interactions or non-linear terms with a reduced predictor set with many near-zero variance predictors being removed to get a more stable model.

Build the logistic regression model (or load the pre-built model). Uncomment the code below if loading the pre-built model is undesirable:
```{r message=FALSE}
#set.seed(1410)
#lrFit <- train(CARAVAN ~ ., data = trainingInd[, noNZVSet],
#              method = "glm",
#              trControl = ctrl,
#              metric = "ROC")
#save(lrFit,file="lrFit.model.rda")
#rm(lrFit)
load(file="lrFit.model.rda")
```

Build the random forest model (or load the pre-built model). Uncomment the code below if loading the pre-built model is undesirable:
```{r message=FALSE}
# set.seed(1410)
# this may take a long time to compute
# final.ntree=100 was used to generate rfFit
# final.ntree <- 1500 
# rfFit <- train(CARAVAN ~ ., data = trainingInd,
#              method = "rf",
#              trControl = ctrl,
#              ntree = final.ntree,
#              tuneLength = 5,
#              metric = "ROC")
# save(rfFit, file="rfFit.model.rda")
# rm(rfFit)
load(file="rfFit.model.rda")
```

# Evaluate Models
Use a data frame to contain the predictions from different models:
```{r}
evalResults <- data.frame(CARAVAN = evaluation$CARAVAN)
evalResults$RF <- predict(rfFit, 
                          newdata = evaluationInd,
                          type = "prob")[,1]
evalResults$LogReg <- predict(lrFit,
                              newdata = evaluationInd[, noNZVSet],
                              type = "prob")[,1]
```

Create the ROC and lift curves from these objects. Uncomment this code to see examples of how to create ROC and lift plots:
```{r}
rfROC <- roc(evalResults$CARAVAN, evalResults$RF,
               levels = rev(levels(evalResults$CARAVAN)))

labels <- c(RF = "Random Forest", LogReg = "Logistic Regression")
lift1 <- lift(CARAVAN ~ RF + LogReg, 
              data = evalResults,
              labels = labels)
#rfROC
#lift1
#plot(rfROC, legacy.axes = TRUE)
#xyplot(lift1, ylab = "%Events Found", 
#       xlab = "%Customers Evaluated",
#       lwd = 2, type = "l")
```

# Alternate Cutoffs
One method to increase the prediction accuracy of the minority class samples is to determine alternative cutoffs for the predicted probabilities. Using the ROC curve is an approach that allows to calculate the sensitivity and specificity across the spectrum of cutoffs. Thus, an appropriate balance between sensitivity and specificity can be determined. 

Keep in mind that the core of the model does *not* change. The same model parameters are being used. *Alternative Cutoffs method only allows for making trade-offs between particular types of errors*. In other words, in a confusion matrix, it only moves the samples up and down the rows of the matrix but not move the from the off-diagonals to the diagonals because it does not induce any further separation between the classes. Another point to make is that Alternate Cutoff will not work for predictive models that only produce discrete class predictions rather than class probabilities.

To determine the desirable cutoff, one may employ a number of techniques. Note that independent *evaluation* data set should be used for this purpose:

* If the target value for either specificity or sensitivity is known, then the ROC curve will provide the corresponding cutoff
* Alternatively, identifying the point on the ROC curve that is closest (i.e., shortest distance) to the perfect model (100% specificity and 100% sensitivity), which is in the upper left corner of the ROC plot.
* Finally, the Youden's *J* index (*J = Sensitivity + Specificity -1*) can be used to measure the proportion of correctly predicted samples for both the event and non-event groups. The largest index across all the cutoffs on the ROC curve will then be the best cutoff for superior model performance.

Given the ROC curve, the *pROC* package allows to investigate possible cutoffs. The `coords` function returns the points on the ROC curve as well as deriving new cutoffs. 

The main arguments are `x`, which specifies what should be returned. A value of `x = "all"` will return the coordinates for the curve and their associated cutoffs. A value of "best" will derive a new cutoff. Using `x = "best"` in conjunction with the `best.method` (either `"youden"` or `"closest.topleft"`) can be informative:

```{r}
rfThresh <- coords(rfROC, x = "best", best.method = "closest.topleft")
rfThresh
```

For this selected cutoff, the new predictions are then computed as:
```{r}
newValue <- factor(ifelse(evalResults$RF > rfThresh,
                          "insurance", "noinsurance"),
                   levels = levels(evalResults$CARAVAN))
```

# Sampling Methods
If the *training data set* is sampled to be *balanced* (via oversampling or undersampling), then the *evaluation* and *test data sets* should be sampled to be more consistent with the state of nature and *should reflect the imbalance* so that estimated of future performance can be properly computed.

Two common *post-hoc* sampling approaches are:

* *down-sampling*: reduces the number of samples to improve balance across classes (e.g., selecting data points from the majority class till it becomes roughly the same size as the minory class)
* *up-sampling*: imputes additional data points to improve balance across classes (e.g., up-sampling from the minority class with replacement by bringing the minority class to equal to the majority)

The `caret` package has two functions, `downSample` and `upSample`, that readjust the class frequencies. Each takes arguments for the predictors (called `x`) and the outcome class (`y`). Both functions return a data frame with the sampled version of the training set:

```{r}
set.seed(1103)
predictors <- noNZVSet
upSampledTrain <- upSample(x = trainingInd[,predictors],
                           y = trainingInd$CARAVAN,
                          yname = "CARAVAN")
dim(training)
dim(upSampledTrain)
table(upSampledTrain$CARAVAN)
```

The down-sampling function has the same syntax. A function for `SMOTE`
can be found in the `DMwR` package. It takes a model formula as an input,
along with parameters (such as the amount of over- and under-sampling and
the number of neighbors). The basic syntax is

```{r}
set.seed(1103)
smoteTrain <- SMOTE(CARAVAN ~ ., data = trainingInd)
dim(smoteTrain)
table(smoteTrain$CARAVAN)
```

These data sets can be used as inputs into the previous modeling code.

# Cost-Sensitive Training with SVM Models
Instead of optimizing the typical performance measure such as accuracy or impurity, *cost-sensitive models* optimize **a cost or loss function** by differently weighing specific types of errors or by associating different costs with specific classes. For example, misclassifying true events (false negatives) may be X times as costly as incorrectly predicting nonevents (false positives). Unlike alternate cutoff methods, *cost-sensitive models indeed affect the model parameters and thus have the potential to improve the classifier*.

For support vector machine (**SVM**) models, *costs are associated with specific classes*. Class-weighted SVMs can be created using the `kernlab` package. The syntax
for the `ksvm` function is the same as previous descriptions, but the
`class.weights` argument is put to use. The train function has similar syntax. We will train over a large cost range, so we precompute the sigma
parameter and make a custom tuning grid. Class probabilities cannot be generated with class weights, so use the control object `ctrlNoProb` to avoid estimating the ROC curve. For prediction,
use the same syntax as for unweighted models.

```{r}
#set.seed(1157)
#sigma <- sigest(CARAVAN ~ ., data = trainingInd[, noNZVSet], frac = .75)
#names(sigma) <- NULL
#svmGrid <- data.frame(.sigma = sigma[2], .C = 2^seq(-6, 1, length = 15))

#set.seed(1401)
#SVMwts <- train(CARAVAN ~ .,
#                data = trainingInd[, noNZVSet],
#                method = "svmRadial",
#                tuneGrid = svmGrid,
#                preProc = c("center", "scale"),
#                class.weights = c(insurance = 18, noinsurance = 1),
#                metric = "Sens",
#                trControl = ctrlNoProb)
#SVMwts
#save (SVMwts, file="SVMwts.model.rda")
load ("SVMwts.model.rda")
SVMwts
```

# Cost-Sensitive Training with CART Models

Classification tree models, such as CART and C5.0 trees, can incorporate costs in a number of different ways:

* The cost of the particular mistake
* The probability of making the mistake
* The prior probability of the classes

For cost-sensitive CART models, the `rpart` package is used with the parms
argument, which is a list of fitting options. One option, loss, can take a
matrix of costs:

```{r}
costMatrix <- matrix(c(0, 1, 20, 0), ncol = 2)
rownames(costMatrix) <- levels(trainingInd$CARAVAN)
colnames(costMatrix) <- levels(trainingInd$CARAVAN)
costMatrix
```

Here, there would be a 20-fold higher cost of a false negative than a false
positive. To fit the model:
```{r}
#set.seed(1401)
#cartCosts <- train(x = trainingInd[,predictors],
#                   y = trainingInd$CARAVAN,
#                   method = "rpart",
#                   trControl = ctrlNoProb,
#                   metric = "Kappa",
#                   tuneLength = 10,
#                   parms = list(loss = costMatrix))
#save (cartCosts, file="cartCosts.model.rda")
load ("cartCosts.model.rda")
cartCosts
```

Similar to the support vector machine model, the syntax for generating class
predictions is the same as the nominal model. However, any class probabilities generated from this model may not match the predicted classes (which is a function of the cost and the probabilities).

`C5.0` has similar syntax to `rpart` by taking a cost matrix, although this
function uses the transpose of the cost matrix structure used by `rpart`:
```{r}
c5Matrix <- matrix(c(0, 20, 1, 0), ncol = 2)
rownames(c5Matrix) <- levels(trainingInd$CARAVAN)
colnames(c5Matrix) <- levels(trainingInd$CARAVAN)
c5Matrix

#set.seed(1401)
#C5Cost <- train(x = trainingInd[, predictors],
#                y = trainingInd$CARAVAN,
#                method = "C5.0",
#                metric = "Kappa",
#                cost = c5Matrix,
#                trControl = ctrlNoProb)
# save (C5Cost, file="C5Cost.model.rda")
load ("C5Cost.model.rda")
C5Cost
```

When employing costs, the predict function for this model only produces the
discrete classes (i.e., no probabilities).
